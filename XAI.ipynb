{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777d97e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = iris.target\n",
    "\n",
    "\n",
    "model = SVC(kernel='linear', probability=True)  # Ensure the model has predict_proba method for LIME\n",
    "# model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "def explain_instance_with_lime(model, instance_index, X):\n",
    "    \"\"\"\n",
    "    This function explains a single data point's prediction using LIME on the Iris dataset.\n",
    "\n",
    "    Args:\n",
    "        model: Trained machine learning model (must have a predict_proba method).\n",
    "        instance_index: Index of the data point to explain (within the Iris dataset).\n",
    "        X: The Iris features data (pandas DataFrame).\n",
    "\n",
    "    Returns:\n",
    "        LIME explanation for the model's prediction on the chosen data point.\n",
    "    \"\"\"\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        training_data=X.values, feature_names=X.columns.tolist(), class_names=iris.target_names, mode='classification'\n",
    "    )\n",
    "    instance = X.iloc[instance_index].values.reshape(1, -1)\n",
    "    explanation = explainer.explain_instance(instance[0], model.predict_proba)\n",
    "    return explanation.as_list()\n",
    "\n",
    "def explain_instance_with_shap(model, instance_index, X, background_sample_size=100):\n",
    "    \"\"\"\n",
    "    This function explains a single data point's prediction using SHAP on the Iris dataset.\n",
    "\n",
    "    Args:\n",
    "        model: Trained machine learning model (must have a predict method).\n",
    "        instance_index: Index of the data point to explain (within the Iris dataset).\n",
    "        X: The Iris features data (pandas DataFrame).\n",
    "        background_sample_size: Number of samples to use for SHAP background dataset.\n",
    "\n",
    "    Returns:\n",
    "        SHAP explanation for the model's prediction on the chosen data point.\n",
    "    \"\"\"\n",
    "    # Summarize the background using K-means or random sampling\n",
    "    background = shap.kmeans(X, background_sample_size)\n",
    "    explainer = shap.KernelExplainer(model.predict, background)\n",
    "    instance = X.iloc[instance_index].values.reshape(1, -1)\n",
    "    shap_values = explainer.shap_values(instance)\n",
    "    return explainer.expected_value, shap_values, instance\n",
    "\n",
    "def visualize_lime_explanation(explanation):\n",
    "    \"\"\"\n",
    "    Visualizes the LIME explanation using a bar chart and provides detailed explanations.\n",
    "\n",
    "    Args:\n",
    "        explanation: LIME explanation object.\n",
    "    \"\"\"\n",
    "    if not isinstance(explanation, list) or not all(isinstance(i, tuple) for i in explanation):\n",
    "        raise ValueError(\"Explanation must be a list of tuples\")\n",
    "\n",
    "    # Extract feature names and their contributions\n",
    "    features = [feature for feature, weight in explanation]\n",
    "    contributions = [weight for feature, weight in explanation]\n",
    "\n",
    "    # Create a bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.barh(features, contributions, color='skyblue')\n",
    "    plt.xlabel('Contribution to Prediction')\n",
    "    plt.title('LIME Explanation for Instance')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Add values on the bars\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(width, bar.get_y() + bar.get_height() / 2,\n",
    "                 f'{width:.2f}', ha='left', va='center')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Provide detailed explanations\n",
    "    print(\"\\nDetailed Explanation:\")\n",
    "    for feature, contribution in explanation:\n",
    "        if contribution > 0:\n",
    "            print(f\"The feature '{feature}' has a positive contribution of {contribution:.2f}, meaning it supports the model's prediction.\")\n",
    "        else:\n",
    "            print(f\"The feature '{feature}' has a negative contribution of {contribution:.2f}, meaning it opposes the model's prediction.\")\n",
    "\n",
    "def visualize_shap_explanation(expected_value, shap_values, instance, feature_names, plot_type='force'):\n",
    "    \"\"\"\n",
    "    Visualizes the SHAP explanation using various plots.\n",
    "\n",
    "    Args:\n",
    "        expected_value: SHAP expected value (base value).\n",
    "        shap_values: SHAP values array.\n",
    "        instance: The instance being explained.\n",
    "        feature_names: List of feature names.\n",
    "        plot_type: Type of SHAP plot ('force', 'summary', 'dependence').\n",
    "    \"\"\"\n",
    "    shap.initjs()\n",
    "\n",
    "    if plot_type == 'force':\n",
    "        shap.force_plot(expected_value, shap_values[0], instance, feature_names=feature_names, matplotlib=True)\n",
    "        plt.show()\n",
    "    elif plot_type == 'summary':\n",
    "        shap.summary_plot(shap_values, instance, feature_names=feature_names)\n",
    "    elif plot_type == 'dependence':\n",
    "        for i in range(len(feature_names)):\n",
    "            shap.dependence_plot(i, shap_values, instance, feature_names=feature_names)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid plot type. Choose 'force', 'summary', or 'dependence'.\")\n",
    "\n",
    "# Example usage\n",
    "instance_index = 0          # Index of the data point to explain (within the Iris dataset)\n",
    "shap_plot_type = 'summary'  # Choose 'force', 'summary', or 'dependence'\n",
    "\n",
    "# Get LIME explanation\n",
    "lime_explanation = explain_instance_with_lime(model, instance_index, X)\n",
    "visualize_lime_explanation(lime_explanation)\n",
    "\n",
    "# Get SHAP explanation\n",
    "expected_value, shap_values, instance = explain_instance_with_shap(model, instance_index, X)\n",
    "visualize_shap_explanation(expected_value, shap_values, instance, X.columns.tolist(), plot_type=shap_plot_type)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
